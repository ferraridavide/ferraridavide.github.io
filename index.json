[{"content":"In the summer off 2020 I was wrapping up my first year of university. Like most of my classmates, I was still adjusting to this new environment and trying to find my way though classes and exams. It was a weird timeâ€”everything felt new, and on top of that, the world was in the middle of a global pandemic.\nI first heard about Maxidata through word of mouthâ€”a friend mentioned they were looking for a software developer. Curious, I decided to get in touch, and interestingly enough, they required some of the same technologies I had just started teaching myself during the lockdown.\nWith the world at a standstill, I had taken the opportunity to dive into something new. I started experimenting with React and Flutter, building small projects, breaking things just to fix them. I also had some experience with C# and WPF from previous projects, which happened to be a big part of their tech stack, it felt like a sign.\nI didnâ€™t have much of a CVâ€”just a few personal projects, a strong curiosity, and a willingness to learn. But I thought, \u0026ldquo;I have nothing to lose and everything to learn,\u0026rdquo; so I just went for it.\nA few days later, I got a reply.\nThe interview itself was straightforwardâ€”just me talking about a few projects I\u0026rsquo;d built and what I hoped to learn. Soon enough, they offered me the position. That\u0026rsquo;s when I had to make a real choice.\nThis wasnâ€™t a summer internship. This was an actual job. I would have to balance work and university, something none of my friends were doing. A part of me wondered if I was making a mistake and biting off more than I could chew, overloading myself and making university unnecessarily difficult.\nWhat made all the difference was how flexible Maxidata was and has been. They understood that I was a student first. They let me start with just 8 hours a week, giving me room to adjust. That trust gave me the confidence to take the leap.\nI still remember my first day. It was a quiet summer morning, and the office was empty except for me and Federico, who would soon become my mentor. He sat me down at his desk, pulled out a pen and paper, and started drawing. Three simple squares appeared on the blank sheet, connected by arrows.\n\u0026ldquo;This is our SQL database,\u0026rdquo; he explained, drawing the first square. \u0026ldquo;This is our backend, also referred to as the BLL, and this is the WPF client.\u0026rdquo;\nThat was my introduction to uve (short for uve2k.blue), Maxidata\u0026rsquo;s flagship product. We spent that morning going through the fundamentalsâ€”how the system was structured, how data moved through it, and how all the pieces fit together.\nThe best part about working at Maxidata has been the learning curve. Some days were harder than others, sure, but it never felt overwhelming. Many times over the years, I felt I was being challengedâ€”but in a good way, in a way that was rewarding, difficult but satisfying, and ultimately fulfilling. Especially in the early days, Iâ€™d get home feeling like I had learned something new. Work was fun, and I\u0026rsquo;d often realize it was time to leave only because others were starting to pack up.\nThereâ€™s a saying:\n\u0026ldquo;Find a job you love, and you will never have to work a day in your life.\u0026rdquo;\nI always thought that was an exaggeration. Turns out, itâ€™s not.\nBecause we were a small team, I got to work on almost everythingâ€”frontend, backend, database, migrations, architecture, deployment, high-level planning. The more time passed, the more I felt like my voice was heard and my opinions mattered in decision-making. It wasnâ€™t just about writing code; it was about understanding the system, shaping it, and making meaningful improvements.\nWhat I Built Over the years, I got to work on some really cool projects.\nThe biggest one, by far, was the new Blazor-based frontend, which is expected to be part of the next evolution of uve as Maxidata moves away from its legacy desktop client and transitions to the web. Under the technical lead of Federico, I built this project from scratch and maintained it alone for some timeâ€”until a colleague joined, whose tasks I helped coordinate. This was the project where I had the most technical freedom, making key decisions about how things were structured and built.\nThis new frontend follows a micro-frontend architecture, with a host app and many independent modules. Each module could be developed and deployed separately and enabled based on user licenses. Building this wasnâ€™t just about working on the UI. A lot of backend work had to happen too. We had to move from SOAP services to REST, design new APIs in a consistent way, and make sure they handled things like pagination and caching properly. Every day, Iâ€™d bounce between frontend and backendâ€”writing new services, making sure they worked smoothly with the client, and keeping performance in check.\nSpeaking of which, one of the biggest performance challenges came from visualizing big datasets. Our clients often need to display tens of thousands of rows in grid components, which quickly exposed the limits of existing component libraries (and of the frameworkâ€”Blazor still has a long way to go). We tested different options, but none of them performed well enough. So, I decided to bite the bullet and build a custom grid component from scratch, with support for sorting, filtering, and virtualization. To keep it fast, I had to make sure re-renders were done surgicallyâ€”only updating the bare minimum parts instead of rerendering entire sections. This made a huge difference in keeping the UI smooth and responsive.\nI also worked on Analyzer, a module for uve that aids in the creation of customer-specific reports and statistics, cutting down on one-off feature requests. It\u0026rsquo;s also used to expose data to Power BI.\nI helped the team migrate from TFS to Git, which, if youâ€™ve ever dealt with TFS, youâ€™ll know was a much-needed change. I worked on integrating telemetry into our backend, helped configure CI/CD pipelines, set up a platform to help with creating and maintaining documentationâ€”both for internal use and for customers, and I even got to experiment with AI-powered solutions, testing ways they could improve both internal processes and the product itself.\nHelped my team start to transition my companyâ€™s main products code base from TFS to Git, resulting in a first commit of ~18k files and ~5.3M LOC added ðŸ¤¯ pic.twitter.com/2K1l89BpZb\n\u0026mdash; Davide Ferrari (@frrdavide) July 14, 2024 Looking Back As I wrap up my time at Maxidata, I feel grateful more than anything. This job shaped me in ways I never expected. It gave me hands-on experience that university never could, and at the same time, it helped me appreciate what I was learning at university even more. When I started, I was just a student with a passion for coding and and a strong desire to learn. Now, I leave not just as a better developer but as someone who understands what it means to build something realâ€”something that people rely on every day.\nI stayed for almost five years because Maxidata made it easy to stay. They understood my situation, gave me the freedom to balance work and study, and trusted me to grow into my role. Thatâ€™s rare. And I donâ€™t take it for granted.\nMaxidata was where I took my first steps in the world of software development, and I couldnâ€™t have asked for a better place to start.\nThank you, Maxidata.\n","permalink":"https://ferraridavide.github.io/posts/maxidata/","summary":"In the summer off 2020 I was wrapping up my first year of university. Like most of my classmates, I was still adjusting to this new environment and trying to find my way though classes and exams. It was a weird timeâ€”everything felt new, and on top of that, the world was in the middle of a global pandemic.\nI first heard about Maxidata through word of mouthâ€”a friend mentioned they were looking for a software developer.","title":"My Time at Maxidata"},{"content":"Since starting university, my class schedule has been all over the place, often leaving me with random gaps to fill. Iâ€™d end up wandering through the hallways of Nave, moving from building to building, trying to find an empty classroom where my friends and I could settle in.\nSo when the Web Technologies course asked us to create a project for the final exam, I already knew what I wanted to build.\nI was thinking of something simple that would save me (and other students) the hassle of running around campus trying to find a free room. Thatâ€™s how Aule UniPV came to lifeâ€”a web app that puts all the classroom availability data in one clean, easy-to-use interface. No more wanderingâ€”just a quick look, and you know exactly where to go.\nThe idea Aule UniPV pulls data from the official classroom schedules daily and shows it in a friendly, easy-to-use way. Classrooms are presented as a stack of swipable cards, making it super simple to just swipe through and find what\u0026rsquo;s free.\nIf you need more details, there\u0026rsquo;s a full list of all available classrooms, sorted by availability. You can easily search and filter by features like projectors, electrical outlets, windows, blackboards, and moreâ€”just a swipe away. Aule UniPV helps you see exactly what each classroom has, so you can pick the best spot for your needs. Plus, you can bookmark your favorite classrooms to find them even faster next time.\nThe stack Building Aule UniPV was actually a lot of fun and gave me the opportunity to check out some new platforms and libraries I hadn\u0026rsquo;t worked with before. I got to dive into Supabase, which turned out to be a great backend solution for a simple project like this, especially thanks to its generous free tier. On the front end, I used Framer Motion to create playful interactions like the card swiping feature, which makes browsing classrooms a little more intuitive and enjoyable.\nThe frontend of Aule UniPV was built in React with TailwindCSS for rapid UI prototyping, the idea was to create something that felt responsive and interactive, so the user experience was as smooth as possible. I also used TanStack Table to provide a sophisticated data grid, which makes filtering classrooms fast and effective.\nThe backend is powered by Supabase, which turned out to be a solid choice for this project. It handled authentication, integrating with Google Cloud so that only students from the universitadipavia.it domain could report classroom issues, and edge functions for pulling updated data from the official classroom schedules.\nEvery morning at 6 AM, Aule UniPV automatically fetches the latest schedule information. This setup involves scheduled jobs that trigger queries to pull and update classroom availability data from the official calendars. By leveraging the pg_cron extension for PostgreSQL the system keeps everything in sync, ensuring users always have up-to-date information without manual intervention.\nThe project is hosted on GitHub Pages, with GitHub Actions seamlessly managing deployment. Every push triggers an automatic build and deployment, making the process effortless and allowing for rapid iteration on features and bug fixes.\n","permalink":"https://ferraridavide.github.io/posts/aule-unipv-webapp/","summary":"Since starting university, my class schedule has been all over the place, often leaving me with random gaps to fill. Iâ€™d end up wandering through the hallways of Nave, moving from building to building, trying to find an empty classroom where my friends and I could settle in.\nSo when the Web Technologies course asked us to create a project for the final exam, I already knew what I wanted to build.","title":"Aule UniPV: A simple way to find an empty classroom"},{"content":"Recently, I embarked on a weekend project to create a video compilation of all the intros from the tech show TechLinked (which you can check out here). I figured it wouldn\u0026rsquo;t be too hard and I wouldn\u0026rsquo;t need to write any code, but as it turned out, I ended up having to resort to some Python to get the job done.\nI had my Premiere Pro sequence all set up with all the intros chopped up and ready to go, but when I wanted to add text displaying the name and date of the current clip, I couldn\u0026rsquo;t figure out how to do it in Premiere Pro. I was looking for something similar to After Effects\u0026rsquo; expressions, but unfortunately, this feature is not present in Premiere Pro. I tried everything I could think of, but nada.\nThat\u0026rsquo;s when I had to turn to Python to get the job done. In this blog post, I\u0026rsquo;ll show you how I used Python and Premiere Pro\u0026rsquo;s EDL export feature to make it all happen. This method allows you to quickly and easily add subtitles or captions to your video, without having to manually add them frame by frame.\nHere\u0026rsquo;s what we\u0026rsquo;ll do, we\u0026rsquo;re going to use Premiere Pro\u0026rsquo;s EDL export feature to get a text file with all the necessary information about the cuts in our sequence. Then, we\u0026rsquo;ll use Python to convert that EDL file into an SRT file, a format that Premiere Pro can import as subtitles. If you\u0026rsquo;re unfamiliar with EDL, don\u0026rsquo;t worry â€“ I didn\u0026rsquo;t know what it was before this either. It looks something like this:\nTITLE: [name of the exported sequence] FCM: NON-DROP FRAME 001 AX AA/V C 00:00:00:00 00:00:08:04 00:00:00:00 00:00:08:04 * FROM CLIP NAME: 2022-01-01 [b_s9oeQsNfw].mp4 * AUDIO LEVEL AT 00:00:00:00 IS -0.00 DB (REEL AX A1) * AUDIO LEVEL AT 00:00:00:00 IS -0.00 DB (REEL AX A2) 002 AX AA/V C 00:00:00:00 00:00:11:20 00:00:08:04 00:00:19:24 * FROM CLIP NAME: 2022-01-04 [O8PdzoHyM98].mp4 * AUDIO LEVEL AT 00:00:00:00 IS -0.00 DB (REEL AX A1) * AUDIO LEVEL AT 00:00:00:00 IS -0.00 DB (REEL AX A2) As you can see, it contains information about the edits in the sequence, including the clip name, the start and end timecodes of the clip, and the start and end timecodes of the edit on the timeline.\nAlright, let\u0026rsquo;s get to the code!\nIn Premiere Pro, select the sequence you want to export then go to File \u0026gt; Export \u0026gt; EDL. A dialog will pop up, just press OK and save the EDL file.\nInstall the timecode, edl and srt libraries using pip install timecode edl srt and import them in your Python script\nLet\u0026rsquo;s use the edl library to parse the EDL file.\nparser=edl.Parser(\u0026#39;29.97\u0026#39;) # Set your sequence\u0026#39;s frame rate here with open(\u0026#39;file.edl\u0026#39;) as edl_file: edl=parser.parse(edl_file) events = edl.events This will create a list of Event objects, each representing a single edit in the EDL file.\nIterate through the list of events and create a list of srt.Subtitle objects. By default, the clip_name is the file name of the clip. If you want to display something different in the SRT file, you can create a separate Python function that takes the file name as input and returns the desired text.\ntc = timecode.Timecode(\u0026#39;29.97\u0026#39;) def to_timedelta(time_code): t = tc.parse_timecode(str(time_code)) td = timedelta(hours=t[0], minutes=t[1], seconds=t[2], milliseconds=(t[3]/29.97)*1000) return timedelta(seconds=td.total_seconds()*1.001) def get_text(filename): # Returns the desired text based on the filename return filename subtitles = [] for index, event in enumerate(events, start=1): start_time = to_timedelta(event.rec_start_tc) end_time = to_timedelta(event.rec_end_tc) clip_name = get_text(event.clip_name) subtitles.append(srt.Subtitle(index, start_time, end_time, clip_name)) You may notice that in the to_timedelta function, I\u0026rsquo;m scaling the length of every clip by 1.001, this is a workaround I found as without this, the subtitles wouldn\u0026rsquo;t align properly in the timeline.\nUse the srt library to create the SRT file.\nwith open(\u0026#39;subtitles.srt\u0026#39;, \u0026#39;w\u0026#39;) as srt_file: srt.dump(subtitles, srt_file) And voila! You can now import the .srt file into Premiere Pro and drag in right into your timeline! It should look something like this:\n1 00:00:00,000 --\u0026gt; 00:00:08,141 SSD go brrr 01 January 2022 2 00:00:08,141 --\u0026gt; 00:00:19,820 We thought it wasn\u0026#39;t possible... 04 January 2022 That\u0026rsquo;s it! In my case, every clip filename contained the TechLinked episode\u0026rsquo;s video ID, so i had my get_text function parse the ID using regular expressions, then call YouTube\u0026rsquo;s API to obtain the video title and upload date\nWhile this technique is particularly useful for adding automatic text overlays or subtitles, the possibilities don\u0026rsquo;t stop there. EDL files contain a wealth of information about the edits in your sequence, so you can use this information in many other ways. For example, you could use an EDL file to automatically create YouTube chapters based on the cuts in your sequence.\n","permalink":"https://ferraridavide.github.io/posts/automating-text-premiere-pro/","summary":"Recently, I embarked on a weekend project to create a video compilation of all the intros from the tech show TechLinked (which you can check out here). I figured it wouldn\u0026rsquo;t be too hard and I wouldn\u0026rsquo;t need to write any code, but as it turned out, I ended up having to resort to some Python to get the job done.\nI had my Premiere Pro sequence all set up with all the intros chopped up and ready to go, but when I wanted to add text displaying the name and date of the current clip, I couldn\u0026rsquo;t figure out how to do it in Premiere Pro.","title":"Using Python to automate text overlays in Premiere Pro"},{"content":"One of the reasons that lead me to finally get a Synology NAS was that i needed a simple and reliable way to solve the chaotic photo situation i had going on, all of my and my familyâ€™s photos were spread over multiple hard drives and other mediums, scattered in unorganized random folders, to put it simply, they were all over the place.\nI was looking for a solution that is reliable, easily maintainable, and that would allow me to neatly organize all of my photos in a single place, something that has fine-grained control over photo access permissions with a user system, and with an intuitive enough user interface that my tech-illiterate family members could have easy access to our photos.\nOut of all the other options i looked at, like Plex and PhotoPrism, Synology Photo checked almost all of those boxes.\nI took all of the photos from the many hard drives i collected over the years, wrote a script to organize them in a predictable folder structure based on the photoâ€™s metadata (i went with a simple [year]/[month]/[day] structure) and proceeded to dump them all on the NAS\nOnly one thing left to do: separate my private photos from the ones i want to share with my familyâ€™s account, i though this would be simple enough, i just organize the photos in albums and then move the albums to the desired location (userâ€™s personal space or shared space), bam, done!\nSadly, as it turns out, the Synology Photoâ€™s web interface and application allow you to move a selection of photos from personal space to shared space only if the selection is made from the timeline or folder view, the option is not available in the album views! Also, moving photos this way wouldnâ€™t preserve the folder structure i made as all the files are moved to the same folder, what a bummer!\nTimeline view selection on the left, Album view selection on the right\nSo let the tinkering begin! In order to move the albums we have to gain access to the Synology Photoâ€™s internal database, query the contents of the album, and then use a script to move the contents automatically to the desired destination\nAccessing Synology DSM internal databases Several of Synology DSMâ€™s services, like Download Station and Synology Photo, run on PostgreSQL databases, there are two main ways to access the data stored in these databases:\nDump the whole database to file with pg_dump then copy and inspect the dump locally (for this, i suggest following this thread) Gain access to the internal PostgreSQL server running in DSM and query the database directly In this guide Iâ€™ll show you how to access the database directly, letâ€™s begin!\nGaining access to the internal PostgreSQL server If you havenâ€™t done so already, enable SSH on your Synology NAS by going in Control Panel â†’ Terminal \u0026amp; SNMP â†’ Terminal â†’ Enable SSH service\nIf the firewall is enabled, make sure to open PostgreSQL default port 5432 (TCP)\nUsing the SSH terminal, add this line host all all [Your computer IP address]/32 trust to file /etc/postgresql/pg_hba.conf If your user doesnâ€™t have the necessary permissions to perform this action, run sudo su - postgres It should look something like this:\n# TYPE DATABASE USER ADDRESS METHOD local all postgres peer map=pg_root local all all peer host all all 192.168.1.101/32 trust In /etc/postgresql/postgresql.conf, set this parameter listen_addresses = '*'\nRestart the PostgreSQL server with pg_ctl -m fast restart\nCheck for errors with tail -f /var/log/postgresql.log\nIf everything went well, you should now be able to access the PostgreSQL database with your favorite client! JetBrains DataGrip worked best for me\nExporting Synology Photo albums All thatâ€™s left to do is to query the database and extract the albumâ€™s photos, Iâ€™m going to use JetBrains DataGrip in the following steps, but any other Postgres capable client should be fairly similar\nIn the Database Explorer add a new PostgreSQL Data Source and set these parameters Host: [your Synology NAS IP address]\nAuthentication: User \u0026amp; Password\nUser: postgres\nPassword: [leave blank]\nDatabase: synofoto\nTest the connection and hit Apply\nIn the data source you just created, navigate to synofoto â†’ public â†’ tables, youâ€™ll see the list of all the tables in the Synology Photo database\nRun this query to list every element in every album\nSELECT a.id, a.name as albumName, r.id_item, u.filename, f.name as dirName, CONCAT(f.name,\u0026#39;/\u0026#39;, u.filename) as path FROM public.normal_album AS a JOIN many_item_has_many_normal_album AS r ON a.id = r.id_normal_album JOIN unit AS u ON r.id_item = u.id JOIN folder AS f ON u.id_folder = f.id; You can now export the query result to CSV file! Remember to add column headers, you will need them for your script later\nIn my case, i went for a PowerShell script that reads this CSV file and moves the photos and videos to a new destination\n$data = Import-Csv -Path \u0026#34;[...]\u0026#34; $basepath = \u0026#34;[...]\u0026#34; $dest = \u0026#34;[...]\u0026#34; $data.Where{$_.albumname -eq \u0026#39;[...]\u0026#39;} | ForEach-Object { $fullpath = Join-Path -Path $basepath -ChildPath $_.path Write-Output $fullpath if (Test-Path -Path $fullpath){ $destPath = Join-Path -Path $dest -ChildPath $_.path $destDirectory = Split-Path $destPath if (!(Test-Path $destDirectory)) { New-Item -ItemType Directory -Force -Path $destDirectory } Write-Output \u0026#34;--\u0026gt; $destPath\u0026#34; Move-Item -Path $fullpath -Destination $destPath -WarningAction Inquire -ErrorAction Inquire } } ","permalink":"https://ferraridavide.github.io/posts/accessing-synology-dsm-database/","summary":"One of the reasons that lead me to finally get a Synology NAS was that i needed a simple and reliable way to solve the chaotic photo situation i had going on, all of my and my familyâ€™s photos were spread over multiple hard drives and other mediums, scattered in unorganized random folders, to put it simply, they were all over the place.\nI was looking for a solution that is reliable, easily maintainable, and that would allow me to neatly organize all of my photos in a single place, something that has fine-grained control over photo access permissions with a user system, and with an intuitive enough user interface that my tech-illiterate family members could have easy access to our photos.","title":"Accessing Synology DSM internal databases and exporting Synology Photo albums"},{"content":"C#\nI am highly proficient in C# and possess a deep understanding of its most advanced concepts, I am comfortable navigating complex projects and solutions. My knowledge of C# has allowed me to create a wide range of applications, from desktop software to web applications and services. I am confident in my ability to write robust, high-quality code in C#.\nWPF\nI have experience using WPF to create professional user interfaces for business-oriented desktop applications. I am able to create rich and interactive user interfaces, utilizing features such as data binding, control templates and animation. My experience with WPF has allowed me to build applications with sleek and modern interfaces.\nASP.NET Core\nI am proficient in building web services using ASP.NET Core and have experience with creating and consuming web APIs, using Entity Framework Core for data access and JWT Tokens for authentication. Additionally, I\u0026rsquo;m experienced with various features of ASP.NET Core like routing, middleware, dependency injection, and configuration that enable me to build scalable and high-performance web services.\nMSBuild\nWith MSBuild, I am able to customize and automate the build process by creating specialized build tasks that fit the needs of a project. I am able to navigate MSBuild logs and troubleshoot any issues that may arise during the build process.\nBlazor\nI have experience using Blazor to create modular single-page applications, I am able to build web applications using C# and Razor syntax and I am familiar with its component lifecycle and JavaScript interop features, allowing me to create complex and dynamic applications that run seamlessly in the browser.\nOpenAPI Specification and NSwag\nI am familiar with the OpenAPI Specification (formerly known as Swagger) and have maintained a WCF service to OpenAPI conversion tool. I am able to design and document APIs using a standardized format, allowing for easy integration and consumption of APIs by external clients. My experience with the WCF service to OpenAPI conversion tool has allowed me to assist in the process of migrating existing APIs to the OpenAPI standard. I am also familiar with NSwag and similar tools that allow me to automatically generate client code for accessing APIs, saving time and effort in the development process.\nMicrosoft SQL Server\nI have experience with Microsoft SQL Server and am proficient in SQL in general. I am able to write queries, stored procedures, and migrations. My experience with Microsoft SQL Server has allowed me to manage and analyze data stored in databases, perform data manipulation tasks, and design and implement database structures to support the needs of various applications.\nGit\nI have used Git for version control on multiple software development projects. With Git, I am able to track changes to code, revert back to previous versions if necessary, and collaborate with team members on projects.\nDocker\nI have used Docker and am familiar with the concept of containerization. With Docker, I am able to package applications and their dependencies into lightweight containers that can be easily deployed and run on any host machine. This allows for greater portability and consistency in the development and deployment process. My understanding of containerization allows me to understand the benefits and potential uses of Docker and other containerization technologies.\nRedis\nI have experience using Redis, an in-memory data structure store that can be used as a database, cache, and message broker. With Redis, I am able to store and retrieve data quickly, implement caching to improve the performance of applications, and facilitate communication between processes through the use of its message brokering capabilities. My experience with Redis has allowed me to utilize its powerful features in various projects to improve the scalability and efficiency of applications.\nVPNs and networking\nI have experience configuring VPNs and have a general understanding of networking concepts. With my knowledge, I am able to set up and maintain VPN connections to securely connect to networks and resources remotely. My understanding of networking concepts allows me to have a broad understanding of how networks operate and how to troubleshoot and resolve issues that may arise.\nJavaScript and web standards\nI have experience with JavaScript and am familiar with web standards. With my knowledge of JavaScript, I am able to create interactive and dynamic web pages and add functionality to web applications. My understanding of web standards allows me to create websites that are compatible with a wide range of browsers and devices. I also have some experience with React, a JavaScript library for building user interfaces. My experience with React has allowed me to build reusable and scalable components for web applications.\nFlutter\nI have experience with Flutter, a mobile development framework for creating cross-platform native applications. With Flutter, I am comfortable developing for mobile platforms and have been able to create applications that run on both Android and iOS devices. My experience with Flutter has allowed me to quickly and efficiently develop mobile applications that have a native feel and perform well on a variety of devices.\nAnimation\nI have experience with animation and have used Framer Motion to create front-end eye candy. With my understanding of animation, I am able to add visual interest and motion to web applications, making them more engaging and interactive for users. My experience with Framer Motion has allowed me to utilize its powerful and intuitive animation tools to create dynamic and pleasing visual effects.\nExperimenting ðŸ§ª Technologies i'm currently exploring or would like to learn in the near future Node.js and Express\nI am seeking to expand my knowledge of Node.js and Express, this would allow me to build robust and scalable web services using TypeScript and take advantage of the vast ecosystem of Node.js modules.\nReact and Next.js\nI already have some experience creating basic React applications and am familiar with its core concepts, including building user interfaces using reusable components. However, I am interested in delving deeper into React and Next.js in order to improve the performance of my applications. Specifically, I want to explore techniques such as server-side rendering and partial hydration, which can help speed up the loading time of a page, provide a better user experience, and potentially increase SEO for the website.\n.NET MAUI\nI am eager to combine what I\u0026rsquo;ve learned developing mobile applications with Flutter and my knowledge of XAML gained from writing WPF applications by exploring the capabilities of .NET MAUI. I am interested in learning how to use MAUI to create multi-platform mobile applications with a single codebase, using XAML (or MauiReactor/Blazor Bindings) for designing user interfaces.\n","permalink":"https://ferraridavide.github.io/skills/","summary":"C#\nI am highly proficient in C# and possess a deep understanding of its most advanced concepts, I am comfortable navigating complex projects and solutions. My knowledge of C# has allowed me to create a wide range of applications, from desktop software to web applications and services. I am confident in my ability to write robust, high-quality code in C#.\nWPF\nI have experience using WPF to create professional user interfaces for business-oriented desktop applications.","title":"Skills"},{"content":" Now UniversitÃ  degli Studi di Pavia 2020 - 2023 Zucchetti Maxidata 2020 - 2023 Crypto Telegram Bot Starship's first orbital launch Most powerful rocket ever launched ðŸš€ ChatGPT for PowerToys Run Siak Sistemi Rodeco RADI Moved to London ","permalink":"https://ferraridavide.github.io/timeline/","summary":" Now UniversitÃ  degli Studi di Pavia 2020 - 2023 Zucchetti Maxidata 2020 - 2023 Crypto Telegram Bot Starship's first orbital launch Most powerful rocket ever launched ðŸš€ ChatGPT for PowerToys Run Siak Sistemi Rodeco RADI Moved to London ","title":"Timeline"},{"content":"Here I share quick insights, highlights, and reflections on what I'm currently learning or working on.\nUpdated every few days, these notes provide a snapshot of my ongoing journey, showcasing my progress, challenges, and the intriguing details that capture my interest.\nI hope you find these glimpses into my work as inspiring and informative as I do.\n","permalink":"https://ferraridavide.github.io/updates/","summary":"Here I share quick insights, highlights, and reflections on what I'm currently learning or working on.\nUpdated every few days, these notes provide a snapshot of my ongoing journey, showcasing my progress, challenges, and the intriguing details that capture my interest.\nI hope you find these glimpses into my work as inspiring and informative as I do.","title":"Updates"}]