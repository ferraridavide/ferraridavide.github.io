[{"content":"Recently, I embarked on a weekend project to create a video compilation of all the intros from the tech show TechLinked (which you can check out here). I figured it wouldn\u0026rsquo;t be too hard and I wouldn\u0026rsquo;t need to write any code, but as it turned out, I ended up having to resort to some Python to get the job done.\nI had my Premiere Pro sequence all set up with all the intros chopped up and ready to go, but when I wanted to add text displaying the name and date of the current clip, I couldn\u0026rsquo;t figure out how to do it in Premiere Pro. I was looking for something similar to After Effects\u0026rsquo; expressions, but unfortunately, this feature is not present in Premiere Pro. I tried everything I could think of, but nada.\nThat\u0026rsquo;s when I had to turn to Python to get the job done. In this blog post, I\u0026rsquo;ll show you how I used Python and Premiere Pro\u0026rsquo;s EDL export feature to make it all happen. This method allows you to quickly and easily add subtitles or captions to your video, without having to manually add them frame by frame.\nHere\u0026rsquo;s what we\u0026rsquo;ll do, we\u0026rsquo;re going to use Premiere Pro\u0026rsquo;s EDL export feature to get a text file with all the necessary information about the cuts in our sequence. Then, we\u0026rsquo;ll use Python to convert that EDL file into an SRT file, a format that Premiere Pro can import as subtitles. If you\u0026rsquo;re unfamiliar with EDL, don\u0026rsquo;t worry â€“ I didn\u0026rsquo;t know what it was before this either. It looks something like this:\nTITLE: [name of the exported sequence] FCM: NON-DROP FRAME 001 AX AA/V C 00:00:00:00 00:00:08:04 00:00:00:00 00:00:08:04 * FROM CLIP NAME: 2022-01-01 [b_s9oeQsNfw].mp4 * AUDIO LEVEL AT 00:00:00:00 IS -0.00 DB (REEL AX A1) * AUDIO LEVEL AT 00:00:00:00 IS -0.00 DB (REEL AX A2) 002 AX AA/V C 00:00:00:00 00:00:11:20 00:00:08:04 00:00:19:24 * FROM CLIP NAME: 2022-01-04 [O8PdzoHyM98].mp4 * AUDIO LEVEL AT 00:00:00:00 IS -0.00 DB (REEL AX A1) * AUDIO LEVEL AT 00:00:00:00 IS -0.00 DB (REEL AX A2) As you can see, it contains information about the edits in the sequence, including the clip name, the start and end timecodes of the clip, and the start and end timecodes of the edit on the timeline.\nAlright, let\u0026rsquo;s get to the code!\nIn Premiere Pro, select the sequence you want to export then go to File \u0026gt; Export \u0026gt; EDL. A dialog will pop up, just press OK and save the EDL file.\nInstall the timecode, edl and srt libraries using pip install timecode edl srt and import them in your Python script\nLet\u0026rsquo;s use the edl library to parse the EDL file.\nparser=edl.Parser(\u0026#39;29.97\u0026#39;) # Set your sequence\u0026#39;s frame rate here with open(\u0026#39;file.edl\u0026#39;) as edl_file: edl=parser.parse(edl_file) events = edl.events This will create a list of Event objects, each representing a single edit in the EDL file.\nIterate through the list of events and create a list of srt.Subtitle objects. By default, the clip_name is the file name of the clip. If you want to display something different in the SRT file, you can create a separate Python function that takes the file name as input and returns the desired text.\ntc = timecode.Timecode(\u0026#39;29.97\u0026#39;) def to_timedelta(time_code): t = tc.parse_timecode(str(time_code)) td = timedelta(hours=t[0], minutes=t[1], seconds=t[2], milliseconds=(t[3]/29.97)*1000) return timedelta(seconds=td.total_seconds()*1.001) def get_text(filename): # Returns the desired text based on the filename return filename subtitles = [] for index, event in enumerate(events, start=1): start_time = to_timedelta(event.rec_start_tc) end_time = to_timedelta(event.rec_end_tc) clip_name = get_text(event.clip_name) subtitles.append(srt.Subtitle(index, start_time, end_time, clip_name)) You may notice that in the to_timedelta function, I\u0026rsquo;m scaling the length of every clip by 1.001, this is a workaround I found as without this, the subtitles wouldn\u0026rsquo;t align properly in the timeline.\nUse the srt library to create the SRT file.\nwith open(\u0026#39;subtitles.srt\u0026#39;, \u0026#39;w\u0026#39;) as srt_file: srt.dump(subtitles, srt_file) And voila! You can now import the .srt file into Premiere Pro and drag in right into your timeline! It should look something like this:\n1 00:00:00,000 --\u0026gt; 00:00:08,141 SSD go brrr 01 January 2022 2 00:00:08,141 --\u0026gt; 00:00:19,820 We thought it wasn\u0026#39;t possible... 04 January 2022 That\u0026rsquo;s it! In my case, every clip filename contained the TechLinked episode\u0026rsquo;s video ID, so i had my get_text function parse the ID using regular expressions, then call YouTube\u0026rsquo;s API to obtain the video title and upload date\nWhile this technique is particularly useful for adding automatic text overlays or subtitles, the possibilities don\u0026rsquo;t stop there. EDL files contain a wealth of information about the edits in your sequence, so you can use this information in many other ways. For example, you could use an EDL file to automatically create YouTube chapters based on the cuts in your sequence.\n","permalink":"https://ferraridavide.github.io/posts/automating-text-premiere-pro/","summary":"Recently, I embarked on a weekend project to create a video compilation of all the intros from the tech show TechLinked (which you can check out here). I figured it wouldn\u0026rsquo;t be too hard and I wouldn\u0026rsquo;t need to write any code, but as it turned out, I ended up having to resort to some Python to get the job done.\nI had my Premiere Pro sequence all set up with all the intros chopped up and ready to go, but when I wanted to add text displaying the name and date of the current clip, I couldn\u0026rsquo;t figure out how to do it in Premiere Pro.","title":"Using Python to automate text overlays in Premiere Pro"},{"content":"One of the reasons that lead me to finally get a Synology NAS was that i needed a simple and reliable way to solve the chaotic photo situation i had going on, all of my and my familyâ€™s photos were spread over multiple hard drives and other mediums, scattered in unorganized random folders, to put it simply, they were all over the place.\nI was looking for a solution that is reliable, easily maintainable, and that would allow me to neatly organize all of my photos in a single place, something that has fine-grained control over photo access permissions with a user system, and with an intuitive enough user interface that my tech-illiterate family members could have easy access to our photos.\nOut of all the other options i looked at, like Plex and PhotoPrism, Synology Photo checked almost all of those boxes.\nI took all of the photos from the many hard drives i collected over the years, wrote a script to organize them in a predictable folder structure based on the photoâ€™s metadata (i went with a simple [year]/[month]/[day] structure) and proceeded to dump them all on the NAS\nOnly one thing left to do: separate my private photos from the ones i want to share with my familyâ€™s account, i though this would be simple enough, i just organize the photos in albums and then move the albums to the desired location (userâ€™s personal space or shared space), bam, done!\nSadly, as it turns out, the Synology Photoâ€™s web interface and application allow you to move a selection of photos from personal space to shared space only if the selection is made from the timeline or folder view, the option is not available in the album views! Also, moving photos this way wouldnâ€™t preserve the folder structure i made as all the files are moved to the same folder, what a bummer!\nTimeline view selection on the left, Album view selection on the right\nSo let the tinkering begin! In order to move the albums we have to gain access to the Synology Photoâ€™s internal database, query the contents of the album, and then use a script to move the contents automatically to the desired destination\nAccessing Synology DSM internal databases Several of Synology DSMâ€™s services, like Download Station and Synology Photo, run on PostgreSQL databases, there are two main ways to access the data stored in these databases:\nDump the whole database to file with pg_dump then copy and inspect the dump locally (for this, i suggest following this thread) Gain access to the internal PostgreSQL server running in DSM and query the database directly In this guide Iâ€™ll show you how to access the database directly, letâ€™s begin!\nGaining access to the internal PostgreSQL server If you havenâ€™t done so already, enable SSH on your Synology NAS by going in Control Panel â†’ Terminal \u0026amp; SNMP â†’ Terminal â†’ Enable SSH service\nIf the firewall is enabled, make sure to open PostgreSQL default port 5432 (TCP)\nUsing the SSH terminal, add this line host all all [Your computer IP address]/32 trust to file /etc/postgresql/pg_hba.conf If your user doesnâ€™t have the necessary permissions to perform this action, run sudo su - postgres It should look something like this:\n# TYPE DATABASE USER ADDRESS METHOD local all postgres peer map=pg_root local all all peer host all all 192.168.1.101/32 trust In /etc/postgresql/postgresql.conf, set this parameter listen_addresses = '*'\nRestart the PostgreSQL server with pg_ctl -m fast restart\nCheck for errors with tail -f /var/log/postgresql.log\nIf everything went well, you should now be able to access the PostgreSQL database with your favorite client! JetBrains DataGrip worked best for me\nExporting Synology Photo albums All thatâ€™s left to do is to query the database and extract the albumâ€™s photos, Iâ€™m going to use JetBrains DataGrip in the following steps, but any other Postgres capable client should be fairly similar\nIn the Database Explorer add a new PostgreSQL Data Source and set these parameters Host: [your Synology NAS IP address]\nAuthentication: User \u0026amp; Password\nUser: postgres\nPassword: [leave blank]\nDatabase: synofoto\nTest the connection and hit Apply\nIn the data source you just created, navigate to synofoto â†’ public â†’ tables, youâ€™ll see the list of all the tables in the Synology Photo database\nRun this query to list every element in every album\nSELECT a.id, a.name as albumName, r.id_item, u.filename, f.name as dirName, CONCAT(f.name,\u0026#39;/\u0026#39;, u.filename) as path FROM public.normal_album AS a JOIN many_item_has_many_normal_album AS r ON a.id = r.id_normal_album JOIN unit AS u ON r.id_item = u.id JOIN folder AS f ON u.id_folder = f.id; You can now export the query result to CSV file! Remember to add column headers, you will need them for your script later\nIn my case, i went for a PowerShell script that reads this CSV file and moves the photos and videos to a new destination\n$data = Import-Csv -Path \u0026#34;[...]\u0026#34; $basepath = \u0026#34;[...]\u0026#34; $dest = \u0026#34;[...]\u0026#34; $data.Where{$_.albumname -eq \u0026#39;[...]\u0026#39;} | ForEach-Object { $fullpath = Join-Path -Path $basepath -ChildPath $_.path Write-Output $fullpath if (Test-Path -Path $fullpath){ $destPath = Join-Path -Path $dest -ChildPath $_.path $destDirectory = Split-Path $destPath if (!(Test-Path $destDirectory)) { New-Item -ItemType Directory -Force -Path $destDirectory } Write-Output \u0026#34;--\u0026gt; $destPath\u0026#34; Move-Item -Path $fullpath -Destination $destPath -WarningAction Inquire -ErrorAction Inquire } } ","permalink":"https://ferraridavide.github.io/posts/accessing-synology-dsm-database/","summary":"One of the reasons that lead me to finally get a Synology NAS was that i needed a simple and reliable way to solve the chaotic photo situation i had going on, all of my and my familyâ€™s photos were spread over multiple hard drives and other mediums, scattered in unorganized random folders, to put it simply, they were all over the place.\nI was looking for a solution that is reliable, easily maintainable, and that would allow me to neatly organize all of my photos in a single place, something that has fine-grained control over photo access permissions with a user system, and with an intuitive enough user interface that my tech-illiterate family members could have easy access to our photos.","title":"Accessing Synology DSM internal databases and exporting Synology Photo albums"},{"content":"C#\nI am highly proficient in C# and possess a deep understanding of its most advanced concepts, I am comfortable navigating complex projects and solutions. My knowledge of C# has allowed me to create a wide range of applications, from desktop software to web applications and services. I am confident in my ability to write robust, high-quality code in C#.\nWPF\nI have experience using WPF to create professional user interfaces for business-oriented desktop applications. I am able to create rich and interactive user interfaces, utilizing features such as data binding, control templates and animation. My experience with WPF has allowed me to build applications with sleek and modern interfaces.\nASP.NET Core\nI am proficient in building web services using ASP.NET Core and have experience with creating and consuming web APIs, using Entity Framework Core for data access and JWT Tokens for authentication. Additionally, I\u0026rsquo;m experienced with various features of ASP.NET Core like routing, middleware, dependency injection, and configuration that enable me to build scalable and high-performance web services.\nMSBuild\nWith MSBuild, I am able to customize and automate the build process by creating specialized build tasks that fit the needs of a project. I am able to navigate MSBuild logs and troubleshoot any issues that may arise during the build process.\nBlazor\nI have experience using Blazor to create modular single-page applications, I am able to build web applications using C# and Razor syntax and I am familiar with it\u0026rsquo;s component lifecycle and JavaScript interop features, allowing me to create complex and dynamic applications that run seamlessly in the browser.\nOpenAPI Specification and NSwag\nI am familiar with the OpenAPI Specification (formerly known as Swagger) and have maintained a WCF service to OpenAPI conversion tool. I am able to design and document APIs using a standardized format, allowing for easy integration and consumption of APIs by external clients. My experience with the WCF service to OpenAPI conversion tool has allowed me to assist in the process of migrating existing APIs to the OpenAPI standard. I am also familiar with NSwag and similar tools that allow me to automatically generate client code for accessing APIs, saving time and effort in the development process.\nMicrosoft SQL Server\nI have experience with Microsoft SQL Server and am proficient in SQL in general. I am able to write queries, stored procedures, and migrations. My experience with Microsoft SQL Server has allowed me to manage and analyze data stored in databases, perform data manipulation tasks, and design and implement database structures to support the needs of various applications.\nGit\nI have used Git for version control on multiple software development projects. With Git, I am able to track changes to code, revert back to previous versions if necessary, and collaborate with team members on projects.\nDocker\nI have used Docker and am familiar with the concept of containerization. With Docker, I am able to package applications and their dependencies into lightweight containers that can be easily deployed and run on any host machine. This allows for greater portability and consistency in the development and deployment process. My understanding of containerization allows me to understand the benefits and potential uses of Docker and other containerization technologies.\nRedis\nI have experience using Redis, an in-memory data structure store that can be used as a database, cache, and message broker. With Redis, I am able to store and retrieve data quickly, implement caching to improve the performance of applications, and facilitate communication between processes through the use of its message brokering capabilities. My experience with Redis has allowed me to utilize its powerful features in various projects to improve the scalability and efficiency of applications.\nVPNs and networking\nI have experience configuring VPNs and have a general understanding of networking concepts. With my knowledge, I am able to set up and maintain VPN connections to securely connect to networks and resources remotely. My understanding of networking concepts allows me to have a broad understanding of how networks operate and how to troubleshoot and resolve issues that may arise.\nJavaScript and web standards\nI have experience with JavaScript and am familiar with web standards. With my knowledge of JavaScript, I am able to create interactive and dynamic web pages and add functionality to web applications. My understanding of web standards allows me to create websites that are compatible with a wide range of browsers and devices. I also have some experience with React, a JavaScript library for building user interfaces. My experience with React has allowed me to build reusable and scalable components for web applications.\nFlutter\nI have experience with Flutter, a mobile development framework for creating cross-platform native applications. With Flutter, I am comfortable developing for mobile platforms and have been able to create applications that run on both Android and iOS devices. My experience with Flutter has allowed me to quickly and efficiently develop mobile applications that have a native feel and perform well on a variety of devices.\nAnimation\nI have experience with animation and have used Framer Motion to create front-end eye candy. With my understanding of animation, I am able to add visual interest and motion to web applications, making them more engaging and interactive for users. My experience with Framer Motion has allowed me to utilize its powerful and intuitive animation tools to create dynamic and pleasing visual effects.\nExperimenting ðŸ§ª Technologies i'm currently exploring or would like to learn in the near future Node.js and Express\nI am seeking to expand my knowledge of Node.js and Express, this would allow me to build robust and scalable web services using TypeScript and take advantage of the vast ecosystem of Node.js modules.\nReact and Next.js\nI already have some experience creating basic React applications and am familiar with its core concepts, including building user interfaces using reusable components. However, I am interested in delving deeper into React and Next.js in order to improve the performance of my applications. Specifically, I want to explore techniques such as server-side rendering and partial hydration, which can help speed up the loading time of a page, provide a better user experience, and potentially increase SEO for the website.\n.NET MAUI\nI am eager to combine what I\u0026rsquo;ve learned developing mobile applications with Flutter and my knowledge of XAML gained from writing WPF applications by exploring the capabilities of .NET MAUI. I am interested in learning how to use MAUI to create multi-platform mobile applications with a single codebase, using XAML (or MauiReactor/Blazor Bindings) for designing user interfaces.\n","permalink":"https://ferraridavide.github.io/skills/","summary":"C#\nI am highly proficient in C# and possess a deep understanding of its most advanced concepts, I am comfortable navigating complex projects and solutions. My knowledge of C# has allowed me to create a wide range of applications, from desktop software to web applications and services. I am confident in my ability to write robust, high-quality code in C#.\nWPF\nI have experience using WPF to create professional user interfaces for business-oriented desktop applications.","title":"Skills"}]